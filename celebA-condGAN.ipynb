{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Conditional Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import tarfile\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "from mxnet.gluon import nn, utils\n",
    "from mxnet import autograd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 256\n",
    "latent_z_size = 100\n",
    "\n",
    "width = 176\n",
    "height = 220\n",
    "mmean=113.48657\n",
    "mstd=73.67449*2\n",
    "\n",
    "use_gpu = True\n",
    "ctx = mx.gpu() if use_gpu else mx.cpu()\n",
    "\n",
    "lr = 0.0002\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CelebA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augs = mx.image.CreateAugmenter(data_shape=(3, height, width),mean=nd.array([mmean,mmean,mmean]),std=nd.array([mstd,mstd,mstd]))\n",
    "augs.append(mx.image.ResizeAug(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mx.image.ImageIter(\n",
    "    path_imgrec = 'dataset/celeba_train.rec',\n",
    "    path_imgidx = 'dataset/celeba_train.idx',\n",
    "    path_imglist = 'dataset/celeba_train.lst',\n",
    "    data_shape = (3, 80, 64),\n",
    "    label_width = 40,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    aug_list=augs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(img_arr):\n",
    "    plt.imshow(((img_arr.asnumpy().transpose(1, 2, 0) + 1.0) * 127.5).astype(np.uint8))\n",
    "    plt.axis('off')\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    visualize(train_data.next().data[0][i + 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the generator\n",
    "nc = 3\n",
    "ngf = 64\n",
    "netG = nn.Sequential()\n",
    "with netG.name_scope():\n",
    "    # input is Z, going into a convolution\n",
    "    netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n",
    "    netG.add(nn.BatchNorm())\n",
    "    netG.add(nn.Activation('relu'))\n",
    "    # state size. (ngf*8) x 4 x 4\n",
    "    netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, (0,1), use_bias=False))\n",
    "    netG.add(nn.BatchNorm())\n",
    "    netG.add(nn.Activation('relu'))\n",
    "    # state size. (ngf*8) x 10 x 8\n",
    "    netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n",
    "    netG.add(nn.BatchNorm())\n",
    "    netG.add(nn.Activation('relu'))\n",
    "    # state size. (ngf*8) x 20 x 16\n",
    "    netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n",
    "    netG.add(nn.BatchNorm())\n",
    "    netG.add(nn.Activation('relu'))\n",
    "    # state size. (ngf*8) x 40 x 32\n",
    "    netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n",
    "    netG.add(nn.Activation('tanh'))\n",
    "    # state size. (nc) x 64 x 64\n",
    "\n",
    "# build the discriminator\n",
    "ndf = 64\n",
    "netD = nn.Sequential()\n",
    "with netD.name_scope():\n",
    "    # input is (nc) x 80 x 64\n",
    "    netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))\n",
    "    netD.add(nn.LeakyReLU(0.2))\n",
    "    # state size. (ndf) x 40 x 32\n",
    "\n",
    "netD2 = nn.Sequential()\n",
    "with netD2.name_scope():\n",
    "    netD2.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))\n",
    "    netD2.add(nn.BatchNorm())\n",
    "    netD2.add(nn.LeakyReLU(0.2))\n",
    "    # state size. (ndf) x 20 x 16\n",
    "    netD2.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))\n",
    "    netD2.add(nn.BatchNorm())\n",
    "    netD2.add(nn.LeakyReLU(0.2))\n",
    "    # state size. (ndf) x 10 x 8\n",
    "    netD2.add(nn.Conv2D(ndf * 8, 4, 2, (0,1), use_bias=False))\n",
    "    netD2.add(nn.BatchNorm())\n",
    "    netD2.add(nn.LeakyReLU(0.2))\n",
    "    # state size. (ndf) x 4 x 4\n",
    "    netD2.add(nn.Conv2D(1, 4, 1, 0, use_bias=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "\n",
    "# initialize the generator and the discriminator\n",
    "netG.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "netD.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "netD2.initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "\n",
    "# trainer for the generator and the discriminator\n",
    "trainerG = gluon.Trainer(netG.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "trainerD = gluon.Trainer(netD.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})\n",
    "trainerD2 = gluon.Trainer(netD2.collect_params(), 'adam', {'learning_rate': lr, 'beta1': beta1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "\n",
    "real_label = nd.ones((batch_size,), ctx=ctx)\n",
    "fake_label = nd.zeros((batch_size,),ctx=ctx)\n",
    "\n",
    "def facc(label, pred):\n",
    "    pred = pred.ravel()\n",
    "    label = label.ravel()\n",
    "    return ((pred > 0.5) == label).mean()\n",
    "metric = mx.metric.CustomMetric(facc)\n",
    "\n",
    "stamp =  datetime.now().strftime('%Y_%m_%d-%H_%M')\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "f = open(\"/home/mcy/Dropbox/1,biometrics/condGAN_experiments/condGAN.log\", \"a\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    tic = time.time()\n",
    "    btic = time.time()\n",
    "    train_data.reset()\n",
    "    iter = 0\n",
    "    errD_total = 0\n",
    "    errG_total = 0\n",
    "    for batch in train_data:\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        data = batch.data[0].as_in_context(ctx)\n",
    "        # right attribute label y \n",
    "        label = mx.nd.expand_dims(mx.nd.expand_dims(batch.label[0].as_in_context(ctx),axis=2),axis=3)\n",
    "        # wrong attribute label y_hat\n",
    "        label_hat = mx.nd.random_normal(0, 1, shape=(batch_size, 40, 32), ctx=ctx)\n",
    "        label_hat = (label_hat >= 0)*2 - 1\n",
    "        # latent vector z\n",
    "        latent_z = mx.nd.random_normal(0, 1, shape=(batch_size, latent_z_size, 1, 1), ctx=ctx)\n",
    "        latent_z = mx.nd.concat(latent_z,label,dim=1)\n",
    "        # right discriminator labels\n",
    "        label_d = mx.nd.dot(mx.nd.expand_dims(batch.label[0].as_in_context(ctx),axis=2),mx.nd.ones([1,32],ctx))\n",
    "        label_d = mx.nd.reshape(label_d,shape=[batch_size,1,40,32])\n",
    "        # wrong discriminator labels\n",
    "        label_d_hat = mx.nd.reshape(label_hat,shape=[batch_size,1,40,32])\n",
    "        with autograd.record():\n",
    "            # train with real image\n",
    "            # real images right attributes\n",
    "            output = netD(data)\n",
    "            output = netD2(mx.nd.concat(output,label_d)).reshape((-1, 1))\n",
    "            errD_real = loss(output, real_label)\n",
    "            metric.update([real_label,], [output,])\n",
    "            \n",
    "            # real images wrong attributes\n",
    "            output = netD(data)\n",
    "            output = netD2(mx.nd.concat(output,label_d_hat)).reshape((-1, 1))\n",
    "            errD_real = (errD_real + loss(output, fake_label))/2\n",
    "            metric.update([fake_label,], [output,])\n",
    "\n",
    "            # train with fake image\n",
    "            # fake images right attributes\n",
    "            fake = netG(latent_z)\n",
    "            output = netD(fake.detach())\n",
    "            output = netD2(mx.nd.concat(output,label_d)).reshape((-1, 1))\n",
    "            errD_fake = loss(output, fake_label)\n",
    "            errD = errD_real + errD_fake\n",
    "            errD.backward()\n",
    "            metric.update([fake_label,], [output,])\n",
    "        errD_total = errD_total + nd.mean(errD).asscalar()\n",
    "\n",
    "        trainerD.step(batch.data[0].shape[0])\n",
    "        trainerD2.step(batch.data[0].shape[0])\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        with autograd.record():\n",
    "            fake = netG(latent_z)\n",
    "            output = netD(fake)\n",
    "            output = netD2(mx.nd.concat(output,label_d)).reshape((-1, 1))\n",
    "            errG = loss(output, real_label)\n",
    "            errG.backward()\n",
    "        errG_total = errG_total + nd.mean(errG).asscalar()\n",
    "\n",
    "        trainerG.step(batch.data[0].shape[0])\n",
    "\n",
    "        # Print log infomation every ten batches\n",
    "        # if iter % 10 == 0:\n",
    "        #     name, acc = metric.get()\n",
    "        #     logging.info('speed: {} samples/s'.format(batch_size / (time.time() - btic)))\n",
    "        #     logging.info('discriminator loss = %f, generator loss = %f, binary training acc = %f at iter %d epoch %d' \n",
    "        #              %(errD_total, \n",
    "        #                errD_total, acc, iter, epoch))\n",
    "        iter = iter + 1\n",
    "        btic = time.time()\n",
    "    name, acc = metric.get()\n",
    "    metric.reset()\n",
    "    \n",
    "    f.write('discriminator loss = %f, generator loss = %f, binary training acc = %f at epoch %d \\n' \n",
    "                 %(errD_total, errG_total, acc, epoch))\n",
    "\n",
    "    # logging.info('\\nbinary training acc at epoch %d: %s=%f' % (epoch, name, acc))\n",
    "    # logging.info('time: %f' % (time.time() - tic))\n",
    "\n",
    "    # Visualize one generated image for each epoch\n",
    "    fake_img = fake[0]\n",
    "    visualize(fake_img)\n",
    "    plt.savefig('samples_per_epoch/samples-%d.png'%(epoch),dpi=150)\n",
    "    plt.show()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "Given a trained generator, we can generate some images of faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image = 8\n",
    "# right attribute label y \n",
    "label = mx.nd.zeros([1,40,1,1],ctx)\n",
    "label[:,0,:,:]=1\t#5_o_Clock_Shadow\n",
    "label[:,1,:,:]=-1\t#Arched_Eyebrows\n",
    "label[:,2,:,:]=-1\t#Attractive\n",
    "label[:,3,:,:]=1\t#Bags_Under_Eyes\n",
    "label[:,4,:,:]=-1\t#Bald\n",
    "label[:,5,:,:]=-1\t#Bangs\n",
    "label[:,6,:,:]=-1\t#Big_Lips\n",
    "label[:,7,:,:]=1\t#Big_Nose\n",
    "label[:,8,:,:]=1\t#Black_Hair\n",
    "label[:,9,:,:]=-1\t#Blond_Hair\n",
    "label[:,10,:,:]=-1\t#Blurry\n",
    "label[:,11,:,:]=-1\t#Brown_Hair\n",
    "label[:,12,:,:]=-1\t#Bushy_Eyebrows\n",
    "label[:,13,:,:]=-1\t#Chubby\n",
    "label[:,14,:,:]=-1\t#Double_Chin\n",
    "label[:,15,:,:]=1\t#Eyeglasses\n",
    "label[:,16,:,:]=1\t#Goatee\n",
    "label[:,17,:,:]=-1\t#Gray_Hair\n",
    "label[:,18,:,:]=-1\t#Heavy_Makeup\n",
    "label[:,19,:,:]=-1\t#High_Cheekbones\n",
    "label[:,20,:,:]=1\t#Male\n",
    "label[:,21,:,:]=-1\t#Mouth_Slightly_Open\n",
    "label[:,22,:,:]=-1\t#Mustache\n",
    "label[:,23,:,:]=-1\t#Narrow_Eyes\n",
    "label[:,24,:,:]=-1\t#No_Beard\n",
    "label[:,25,:,:]=-1\t#Oval_Face\n",
    "label[:,26,:,:]=-1\t#Pale_Skin\n",
    "label[:,27,:,:]=-1\t#Pointy_Nose\n",
    "label[:,28,:,:]=-1\t#Receding_Hairline\n",
    "label[:,29,:,:]=-1\t#Rosy_Cheeks\n",
    "label[:,30,:,:]=1\t#Sideburns\n",
    "label[:,31,:,:]=-1\t#Smiling\n",
    "label[:,32,:,:]=1\t#Straight_Hair\n",
    "label[:,33,:,:]=-1\t#Wavy_Hair\n",
    "label[:,34,:,:]=-1\t#Wearing_Earrings\n",
    "label[:,35,:,:]=-1\t#Wearing_Hat\n",
    "label[:,36,:,:]=-1\t#Wearing_Lipstick\n",
    "label[:,37,:,:]=-1\t#Wearing_Necklace\n",
    "label[:,38,:,:]=-1\t#Wearing_Necktie\n",
    "label[:,39,:,:]=1\t#Young"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_image):\n",
    "    # latent vector z\n",
    "    latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
    "    latent_z = mx.nd.concat(latent_z,label,dim=1)\n",
    "    img = netG(latent_z)\n",
    "    plt.subplot(2,4,i+1)\n",
    "    visualize(img[0])\n",
    "    plt.savefig('/home/mcy/Dropbox/1,biometrics/condGAN_experiments/samples.png',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also interpolate along the manifold between images by interpolating linearly between points in the latent space and visualizing the corresponding images. We can see that small changes in the latent space results in smooth changes in generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_image = 12\n",
    "label = mx.nd.random_normal(0, 1, shape=(1, 40), ctx=ctx)\n",
    "label = (label >= 0)*2 - 1\n",
    "label = mx.nd.expand_dims(mx.nd.expand_dims(label,axis=2),axis=3)\n",
    "latent_z = mx.nd.random_normal(0, 1, shape=(1, latent_z_size, 1, 1), ctx=ctx)\n",
    "latent_z = mx.nd.concat(latent_z,label,dim=1)\n",
    "step = 0.05\n",
    "for i in range(num_image):\n",
    "    img = netG(latent_z)\n",
    "    plt.subplot(3,4,i+1)\n",
    "    visualize(img[0])\n",
    "    latent_z += 0.05\n",
    "    plt.savefig('/home/mcy/Dropbox/1,biometrics/condGAN_experiments/interpolation_samples.png',dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG.save_parameters(\"params/condGAN-generator.params\")\n",
    "netD.save_parameters(\"params/condGAN-discriminator.params\")\n",
    "netD2.save_parameters(\"params/condGAN-discriminator2.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
